{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "PROSES ENHANCEMENT DATA CORPUS\n",
      "==================================================\n",
      "\n",
      "1. Memuat data corpus...\n",
      "Berhasil load 2754 dokumen dari corpus.json\n",
      "\n",
      "2. Memuat data Excel...\n",
      "Berhasil load data Excel dengan 2754 baris\n",
      "Kolom yang tersedia: ['Title', 'Authors', 'Advisors', 'Keywords', 'Publisher', 'Abstract', 'Issue Date', 'Link Detail', 'Link PDF', 'BAB 1', 'BAB 2', 'BAB 3', 'BAB 4', 'BAB 5']\n",
      "\n",
      "3. Membuat mapping penulis dari Excel...\n",
      "Dibuat mapping untuk 2748 penulis unik\n",
      "\n",
      "4. Contoh mapping penulis yang dibuat:\n",
      "  - windy fajriah fitri:\n",
      "    Link Detail: https://repository.uinjkt.ac.id/dspace/handle/1234...\n",
      "    Link PDF: https://repository.uinjkt.ac.id/dspace/bitstream/1...\n",
      "  - muhammad taqy pratama putra:\n",
      "    Link Detail: https://repository.uinjkt.ac.id/dspace/handle/1234...\n",
      "    Link PDF: https://repository.uinjkt.ac.id/dspace/bitstream/1...\n",
      "  - rio galeh prayoga:\n",
      "    Link Detail: https://repository.uinjkt.ac.id/dspace/handle/1234...\n",
      "    Link PDF: https://repository.uinjkt.ac.id/dspace/bitstream/1...\n",
      "  - yasmine amalia ismail:\n",
      "    Link Detail: https://repository.uinjkt.ac.id/dspace/handle/1234...\n",
      "    Link PDF: https://repository.uinjkt.ac.id/dspace/bitstream/1...\n",
      "  - nabil adnan rosyadi:\n",
      "    Link Detail: https://repository.uinjkt.ac.id/dspace/handle/1234...\n",
      "    Link PDF: https://repository.uinjkt.ac.id/dspace/bitstream/1...\n",
      "\n",
      "5. Menambahkan link ke corpus data...\n",
      "Tidak ditemukan link untuk penulis: habib najjar\n",
      "Tidak ditemukan link untuk penulis: dhiya ramadhanty nastiti\n",
      "Tidak ditemukan link untuk penulis: gusti bagus surya bhuana\n",
      "Tidak ditemukan link untuk penulis: fatih azhah\n",
      "Tidak ditemukan link untuk penulis: wira yulia lubis\n",
      "Tidak ditemukan link untuk penulis: elshaday siregar\n",
      "Tidak ditemukan link untuk penulis: muhammad ulfan huda\n",
      "Tidak ditemukan link untuk penulis: faccettarial marchel marlissa\n",
      "Tidak ditemukan link untuk penulis: sisilia grace adrin tarigan\n",
      "Tidak ditemukan link untuk penulis: tiara angel ginting\n",
      "Tidak ditemukan link untuk penulis: job paskah sundah\n",
      "Tidak ditemukan link untuk penulis: raisis farah dzakiyyah aliyya\n",
      "Tidak ditemukan link untuk penulis: rahma wati sembiring berahmana\n",
      "Tidak ditemukan link untuk penulis: federika pingkan lasut\n",
      "Tidak ditemukan link untuk penulis: moh fitrah giffari\n",
      "Tidak ditemukan link untuk penulis: gst agung wisnu adi kusuma\n",
      "Tidak ditemukan link untuk penulis: gst putu ary wedangga\n",
      "Tidak ditemukan link untuk penulis: septendinova fiona correia\n",
      "Tidak ditemukan link untuk penulis: ach fawaidi\n",
      "Tidak ditemukan link untuk penulis: muhammad ariff zafri bin amin\n",
      "Tidak ditemukan link untuk penulis: visensa gero\n",
      "Tidak ditemukan link untuk penulis: luh nyoman astagina yati\n",
      "Tidak ditemukan link untuk penulis: gabriela natasha bui lulic costa soares\n",
      "Tidak ditemukan link untuk penulis: raf atun fitriani\n",
      "Tidak ditemukan link untuk penulis: mariana suriani lihung\n",
      "Tidak ditemukan link untuk penulis: gst ayu agung adhya monacika\n",
      "Tidak ditemukan link untuk penulis: dhivvyaa krishnan muthy\n",
      "Tidak ditemukan link untuk penulis: sang ayu sri utami dewi\n",
      "Tidak ditemukan link untuk penulis: pardianta sinaga\n",
      "Tidak ditemukan link untuk penulis: viata silva pinto\n",
      "Tidak ditemukan link untuk penulis: martha edlyn marintan hutagalung\n",
      "Tidak ditemukan link untuk penulis: faizal saputra\n",
      "Tidak ditemukan link untuk penulis: gusti bagus prema pradana\n",
      "Berhasil menambahkan link ke 2721 dari 2754 dokumen\n",
      "\n",
      "6. Contoh dokumen yang sudah ditingkatkan:\n",
      "\n",
      "Dokumen 1:\n",
      "  ID: doc_0\n",
      "  Penulis: windy fajriah fitri\n",
      "  Link Detail: https://repository.uinjkt.ac.id/dspace/handle/1234...\n",
      "  Link PDF: https://repository.uinjkt.ac.id/dspace/bitstream/1...\n",
      "\n",
      "Dokumen 2:\n",
      "  ID: doc_1\n",
      "  Penulis: muhammad taqy pratama putra\n",
      "  Link Detail: https://repository.uinjkt.ac.id/dspace/handle/1234...\n",
      "  Link PDF: https://repository.uinjkt.ac.id/dspace/bitstream/1...\n",
      "\n",
      "Dokumen 3:\n",
      "  ID: doc_2\n",
      "  Penulis: rio galeh prayoga\n",
      "  Link Detail: https://repository.uinjkt.ac.id/dspace/handle/1234...\n",
      "  Link PDF: https://repository.uinjkt.ac.id/dspace/bitstream/1...\n",
      "\n",
      "7. Menyimpan hasil ke database_TA.json...\n",
      "Data berhasil disimpan ke: C:\\Users\\Widnyana\\Documents\\TUGAS AKHIR\\Program TA\\skripsi-search-engine\\data\\processed\\database_TA.json\n",
      "\n",
      "==================================================\n",
      "PROSES SELESAI!\n",
      "Hasil disimpan di: C:\\Users\\Widnyana\\Documents\\TUGAS AKHIR\\Program TA\\skripsi-search-engine\\data\\processed\\database_TA.json\n",
      "Total dokumen: 2754\n",
      "==================================================\n",
      "\n",
      "STATISTIK:\n",
      "  - Dokumen dengan link detail: 2721\n",
      "  - Dokumen dengan link PDF: 168\n",
      "  - Dokumen dengan minimal satu link: 2721\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import unicodedata\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text untuk perbandingan yang lebih baik\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Ubah ke lowercase dan hapus whitespace ekstra\n",
    "    text = text.lower().strip()\n",
    "    # Hapus karakter khusus dan normalisasi\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n",
    "    return text\n",
    "\n",
    "def load_corpus_data(corpus_path):\n",
    "    \"\"\"Load data corpus dari file JSON\"\"\"\n",
    "    try:\n",
    "        with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "            corpus_data = json.load(f)\n",
    "        print(f\"Berhasil load {len(corpus_data)} dokumen dari corpus.json\")\n",
    "        return corpus_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading corpus data: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_excel_data(excel_path):\n",
    "    \"\"\"Load data dari file Excel\"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(excel_path)\n",
    "        print(f\"Berhasil load data Excel dengan {len(df)} baris\")\n",
    "        print(f\"Kolom yang tersedia: {list(df.columns)}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Excel data: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_author_mapping(df):\n",
    "    \"\"\"Buat mapping berdasarkan nama penulis yang dinormalisasi\"\"\"\n",
    "    author_mapping = {}\n",
    "    \n",
    "    # Pastikan kolom yang diperlukan ada\n",
    "    required_columns = ['Authors', 'Link Detail', 'Link PDF']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            print(f\"Kolom '{col}' tidak ditemukan di Excel\")\n",
    "            print(f\"Kolom yang ada: {list(df.columns)}\")\n",
    "            return {}\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        authors = row['Authors']\n",
    "        if pd.isna(authors):\n",
    "            continue\n",
    "            \n",
    "        # Handle multiple authors (bisa dipisah dengan koma atau titik koma)\n",
    "        author_list = str(authors).split(';') if ';' in str(authors) else str(authors).split(',')\n",
    "        \n",
    "        for author in author_list:\n",
    "            normalized_author = normalize_text(author)\n",
    "            if normalized_author:\n",
    "                author_mapping[normalized_author] = {\n",
    "                    'Link Detail': str(row['Link Detail']) if pd.notna(row['Link Detail']) else '',\n",
    "                    'Link PDF': str(row['Link PDF']) if pd.notna(row['Link PDF']) else ''\n",
    "                }\n",
    "    \n",
    "    print(f\"Dibuat mapping untuk {len(author_mapping)} penulis unik\")\n",
    "    return author_mapping\n",
    "\n",
    "def enhance_corpus_data(corpus_data, author_mapping):\n",
    "    \"\"\"Tambahkan link detail dan PDF ke corpus data\"\"\"\n",
    "    enhanced_count = 0\n",
    "    \n",
    "    for doc in corpus_data:\n",
    "        if 'authors' in doc:\n",
    "            doc_authors = doc['authors']\n",
    "            \n",
    "            # Normalisasi nama penulis dari corpus\n",
    "            normalized_doc_authors = normalize_text(doc_authors)\n",
    "            \n",
    "            # Cari kecocokan di mapping\n",
    "            match_found = False\n",
    "            \n",
    "            # Coba cari exact match dulu\n",
    "            if normalized_doc_authors in author_mapping:\n",
    "                match_found = True\n",
    "                links = author_mapping[normalized_doc_authors]\n",
    "            else:\n",
    "                # Coba cari partial match\n",
    "                for author_key in author_mapping.keys():\n",
    "                    if author_key in normalized_doc_authors or normalized_doc_authors in author_key:\n",
    "                        match_found = True\n",
    "                        links = author_mapping[author_key]\n",
    "                        break\n",
    "            \n",
    "            if match_found:\n",
    "                doc['link_detail'] = links['Link Detail']\n",
    "                doc['link_pdf'] = links['Link PDF']\n",
    "                enhanced_count += 1\n",
    "            else:\n",
    "                doc['link_detail'] = ''\n",
    "                doc['link_pdf'] = ''\n",
    "                print(f\"Tidak ditemukan link untuk penulis: {doc_authors}\")\n",
    "    \n",
    "    print(f\"Berhasil menambahkan link ke {enhanced_count} dari {len(corpus_data)} dokumen\")\n",
    "    return corpus_data\n",
    "\n",
    "def save_enhanced_data(corpus_data, output_path):\n",
    "    \"\"\"Simpan data yang sudah diperkaya ke file JSON\"\"\"\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(corpus_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Data berhasil disimpan ke: {output_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving enhanced data: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    # Tentukan path file\n",
    "    base_path = Path(r\"C:\\Users\\Widnyana\\Documents\\TUGAS AKHIR\\Program TA\\skripsi-search-engine\\data\\processed\")\n",
    "    corpus_path = base_path / \"corpus.json\"\n",
    "    excel_path = base_path / \"clean_dataTA_SearchEngine.xlsx\"\n",
    "    output_path = base_path / \"database_TA.json\"\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"PROSES ENHANCEMENT DATA CORPUS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Load data corpus\n",
    "    print(\"\\n1. Memuat data corpus...\")\n",
    "    corpus_data = load_corpus_data(corpus_path)\n",
    "    \n",
    "    if not corpus_data:\n",
    "        print(\"Tidak ada data corpus yang dimuat. Proses dihentikan.\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Load data Excel\n",
    "    print(\"\\n2. Memuat data Excel...\")\n",
    "    df = load_excel_data(excel_path)\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        print(\"Tidak ada data Excel yang dimuat. Proses dihentikan.\")\n",
    "        return\n",
    "    \n",
    "    # Step 3: Buat mapping dari Excel\n",
    "    print(\"\\n3. Membuat mapping penulis dari Excel...\")\n",
    "    author_mapping = create_author_mapping(df)\n",
    "    \n",
    "    if not author_mapping:\n",
    "        print(\"Tidak berhasil membuat mapping penulis. Proses dihentikan.\")\n",
    "        return\n",
    "    \n",
    "    # Step 4: Tampilkan beberapa contoh mapping\n",
    "    print(\"\\n4. Contoh mapping penulis yang dibuat:\")\n",
    "    sample_authors = list(author_mapping.keys())[:5]\n",
    "    for author in sample_authors:\n",
    "        print(f\"  - {author}:\")\n",
    "        print(f\"    Link Detail: {author_mapping[author]['Link Detail'][:50]}...\")\n",
    "        print(f\"    Link PDF: {author_mapping[author]['Link PDF'][:50]}...\")\n",
    "    \n",
    "    # Step 5: Enhance corpus data\n",
    "    print(\"\\n5. Menambahkan link ke corpus data...\")\n",
    "    enhanced_corpus = enhance_corpus_data(corpus_data, author_mapping)\n",
    "    \n",
    "    # Step 6: Tampilkan beberapa contoh hasil\n",
    "    print(\"\\n6. Contoh dokumen yang sudah ditingkatkan:\")\n",
    "    for i, doc in enumerate(enhanced_corpus[:3]):\n",
    "        print(f\"\\nDokumen {i+1}:\")\n",
    "        print(f\"  ID: {doc.get('doc_id', 'N/A')}\")\n",
    "        print(f\"  Penulis: {doc.get('authors', 'N/A')}\")\n",
    "        print(f\"  Link Detail: {doc.get('link_detail', 'N/A')[:50]}...\")\n",
    "        print(f\"  Link PDF: {doc.get('link_pdf', 'N/A')[:50]}...\")\n",
    "    \n",
    "    # Step 7: Simpan hasil\n",
    "    print(\"\\n7. Menyimpan hasil ke database_TA.json...\")\n",
    "    success = save_enhanced_data(enhanced_corpus, output_path)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"PROSES SELESAI!\")\n",
    "        print(f\"Hasil disimpan di: {output_path}\")\n",
    "        print(f\"Total dokumen: {len(enhanced_corpus)}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Statistik\n",
    "        docs_with_links = sum(1 for doc in enhanced_corpus if doc.get('link_detail') or doc.get('link_pdf'))\n",
    "        print(f\"\\nSTATISTIK:\")\n",
    "        print(f\"  - Dokumen dengan link detail: {sum(1 for doc in enhanced_corpus if doc.get('link_detail'))}\")\n",
    "        print(f\"  - Dokumen dengan link PDF: {sum(1 for doc in enhanced_corpus if doc.get('link_pdf'))}\")\n",
    "        print(f\"  - Dokumen dengan minimal satu link: {docs_with_links}\")\n",
    "    else:\n",
    "        print(\"\\nGagal menyimpan hasil.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dokumen: 2754\n",
      "File berhasil dibuat: c:\\Users\\Widnyana\\Documents\\TUGAS AKHIR\\Program TA\\skripsi-search-engine\\data\\processed\\database_skripsi.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# =========================\n",
    "# PATH\n",
    "# =========================\n",
    "excel_path = r\"C:\\Users\\Widnyana\\Documents\\TUGAS AKHIR\\Program TA\\skripsi-search-engine\\data\\processed\\clean_dataTA_SearchEngine.xlsx\"\n",
    "output_json = \"database_skripsi.json\"\n",
    "\n",
    "# =========================\n",
    "# LOAD EXCEL\n",
    "# =========================\n",
    "df = pd.read_excel(excel_path)\n",
    "\n",
    "# =========================\n",
    "# FUNGSI AMAN KONVERSI KE STRING\n",
    "# =========================\n",
    "def safe_str(value):\n",
    "    if pd.isna(value):\n",
    "        return \"\"\n",
    "    if isinstance(value, (pd.Timestamp, datetime)):\n",
    "        return value.strftime(\"%Y-%m-%d\")\n",
    "    return str(value)\n",
    "\n",
    "# =========================\n",
    "# BUILD JSON STRUCTURE\n",
    "# =========================\n",
    "documents = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    doc = {\n",
    "        \"doc_id\": f\"doc_{idx}\",\n",
    "        \"title\": safe_str(row.get(\"Title\", \"\")),\n",
    "        \"keywords\": safe_str(row.get(\"Keywords\", \"\")),\n",
    "        \"abstract\": safe_str(row.get(\"Abstract\", \"\")),\n",
    "        \"authors\": safe_str(row.get(\"Authors\", \"\")),\n",
    "        \"issue_date\": safe_str(row.get(\"Issue Date\", \"\")),\n",
    "        \"publisher\": safe_str(row.get(\"Publisher\", \"\")),\n",
    "        \"fields\": {\n",
    "            \"Title\": safe_str(row.get(\"Title\", \"\")),\n",
    "            \"Keywords\": safe_str(row.get(\"Keywords\", \"\")),\n",
    "            \"Abstract\": safe_str(row.get(\"Abstract\", \"\")),\n",
    "            \"BAB 1\": safe_str(row.get(\"BAB 1\", \"\")),\n",
    "            \"BAB 2\": safe_str(row.get(\"BAB 2\", \"\")),\n",
    "            \"BAB 3\": safe_str(row.get(\"BAB 3\", \"\")),\n",
    "            \"BAB 4\": safe_str(row.get(\"BAB 4\", \"\")),\n",
    "            \"BAB 5\": safe_str(row.get(\"BAB 5\", \"\")),\n",
    "            \"Authors\": safe_str(row.get(\"Authors\", \"\")),\n",
    "            \"Advisors\": safe_str(row.get(\"Advisors\", \"\")),\n",
    "            \"Issue Date\": safe_str(row.get(\"Issue Date\", \"\")),\n",
    "            \"Publisher\": safe_str(row.get(\"Publisher\", \"\"))\n",
    "        },\n",
    "        \"link_detail\": safe_str(row.get(\"Link Detail\", \"\")),\n",
    "        \"link_pdf\": safe_str(row.get(\"Link PDF\", \"\"))\n",
    "    }\n",
    "\n",
    "    documents.append(doc)\n",
    "\n",
    "# =========================\n",
    "# SAVE JSON\n",
    "# =========================\n",
    "with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(documents, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Total dokumen: {len(documents)}\")\n",
    "print(f\"File berhasil dibuat: {os.path.abspath(output_json)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta_colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
